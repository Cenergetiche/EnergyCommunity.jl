{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"C:/Users/Davide/Desktop\" #git/gitdf/EnergyCommunity.jl/run_cloud\"\n",
    "\n",
    "n_iter = 12\n",
    "EC_size_list_enum = [10, 20]  # List of sizes of the EC to test in enum mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using EnergyCommunity\n",
    "using FileIO\n",
    "using HiGHS, Plots\n",
    "using JuMP\n",
    "using Gurobi\n",
    "using Games\n",
    "using TickTock\n",
    "using Combinatorics\n",
    "using DataFrames\n",
    "using JLD2\n",
    "using Latexify, LaTeXStrings\n",
    "using YAML\n",
    "using CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare ENUM_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_enums = Dict(\n",
    "    size_enum=>load(\"$parent_dir/results_paper/enum/enum_simulations_results_$size_enum.jld2\")\n",
    "    for size_enum in EC_size_list_enum\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENUM_MODE: Create reward redistribution of enum modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reward_enum = DataFrame()\n",
    "\n",
    "for EC_size in EC_size_list_enum\n",
    "    df_reward_temp = deepcopy(dict_enums[EC_size][\"df_reward_enum\"])\n",
    "    reward_list = setdiff(names(df_reward_temp), [\"user_set\"])\n",
    "    rename!(df_reward_temp, reward_list .=> reward_list .* \"_$EC_size\")\n",
    "    if nrow(df_reward_enum) == 0\n",
    "        df_reward_enum = df_reward_temp\n",
    "    else\n",
    "        df_reward_enum = outerjoin(df_reward_enum, df_reward_temp, on=:user_set, makeunique=true)\n",
    "    end\n",
    "end\n",
    "\n",
    "# sort by user set\n",
    "sort!(df_reward_enum, :user_set)\n",
    "\n",
    "df_reward_enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENUM_MODE: Create comparison of computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_enum = DataFrame()\n",
    "\n",
    "for EC_size in EC_size_list_enum\n",
    "    df_time_temp = DataFrame(dict_enums[EC_size][\"df_time_enum\"])  # dict_time_enum\n",
    "\n",
    "    df_time_temp[!, 2:end] ./= 3600  # change units to hours\n",
    "\n",
    "    df_time_temp[!, \"EC size\"] = [EC_size]\n",
    "    df_time_temp[!, \"title\"] = [L\"Time [h]\"]\n",
    "    df_time_temp = df_time_temp[!, [\"EC size\"; \"title\"; names(df_time_temp)[1:end-2]]]\n",
    "\n",
    "    if nrow(df_time_enum) == 0\n",
    "        df_time_enum = df_time_temp\n",
    "    else\n",
    "        df_time_enum = vcat(df_time_enum, df_time_temp)\n",
    "    end\n",
    "end\n",
    "\n",
    "df_time_enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare ITER_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_run_simulations_iter = DataFrame([\n",
    "    (EC_size=10, PoolSearchMode=1, PoolSolutions=10, precoal=[1], bestobjstop=true),\n",
    "    (EC_size=10, PoolSearchMode=1, PoolSolutions=50, precoal=[1], bestobjstop=true),\n",
    "    (EC_size=10, PoolSearchMode=1, PoolSolutions=200, precoal=[1], bestobjstop=true),\n",
    "    (EC_size=10, PoolSearchMode=0, PoolSolutions=200, precoal=[1, 2], bestobjstop=true),\n",
    "    (EC_size=10, PoolSearchMode=0, PoolSolutions=200, precoal=[1, 2], bestobjstop=false),\n",
    "    (EC_size=10, PoolSearchMode=0, PoolSolutions=200, precoal=[1, 10], bestobjstop=true),\n",
    "    (EC_size=10, PoolSearchMode=0, PoolSolutions=200, precoal=[1, 10], bestobjstop=false),\n",
    "    (EC_size=10, PoolSearchMode=0, PoolSolutions=200, precoal=[1, 2, 3], bestobjstop=false),\n",
    "    (EC_size=10, PoolSearchMode=0, PoolSolutions=200, precoal=[1, 9, 10], bestobjstop=false),\n",
    "    (EC_size=20, PoolSearchMode=0, PoolSolutions=200, precoal=[1, 20], bestobjstop=false),\n",
    "    (EC_size=50, PoolSearchMode=0, PoolSolutions=200, precoal=[1, 50], bestobjstop=false),\n",
    "    (EC_size=100, PoolSearchMode=0, PoolSolutions=200, precoal=[1, 100], bestobjstop=false),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_iter = Dict(\n",
    "    id_run=>load(\"$parent_dir/results_paper/iter/iter_simulations_results_$id_run.jld2\")\n",
    "    for id_run in 1:n_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITER_MODE: Comparison of reward distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reward_iter = DataFrame()\n",
    "\n",
    "for id_run in 1:n_iter\n",
    "    df_reward_temp = deepcopy(dict_iter[id_run][\"df_reward_iter\"])\n",
    "    reward_list = setdiff(names(df_reward_temp), [\"user_set\"])\n",
    "    rename!(df_reward_temp, reward_list .=> reward_list .* \"_$(nrow(df_reward_temp)-1)\")\n",
    "    if nrow(df_reward_iter) == 0\n",
    "        df_reward_iter = df_reward_temp\n",
    "    else\n",
    "        df_reward_iter = outerjoin(df_reward_iter, df_reward_temp, on=:user_set, makeunique=true)\n",
    "    end\n",
    "end\n",
    "\n",
    "# sort by user set\n",
    "sort!(df_reward_iter, :user_set)\n",
    "CSV.write(\"test.csv\", df_reward_iter)\n",
    "df_reward_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITER_MODE: Create comparison of computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_iter = DataFrame()\n",
    "\n",
    "for id_run in 1:n_iter\n",
    "    df_time_temp = deepcopy(dict_iter[id_run][\"df_time_iter\"])\n",
    "\n",
    "    df_time_temp[!, 3:end] ./= 3600  # change units to hours\n",
    "\n",
    "    df_time_temp[!, \"title\"] = [L\"Time [h]\"]\n",
    "\n",
    "    if nrow(df_time_iter) == 0\n",
    "        df_time_iter = df_time_temp\n",
    "    else\n",
    "        df_time_iter = vcat(df_time_iter, df_time_temp)\n",
    "    end\n",
    "end\n",
    "\n",
    "header_cols = [\"name\", names(df_run_simulations_iter)..., \"title\"]\n",
    "df_list_rewards = setdiff(names(df_time_iter), [\"id_run\"; header_cols])\n",
    "\n",
    "df_time_iter = hcat(df_run_simulations_iter[df_time_iter[!, \"id_run\"], :], df_time_iter[!, df_list_rewards])\n",
    "\n",
    "CSV.write(\"comp_time.csv\", df_time_iter)\n",
    "\n",
    "df_time_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_iter[!, [header_cols; df_list_rewards]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_iter[!, setdiff(names(df_time_iter), [\"id_run\"; header_cols])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcat(df_time_iter[!, setdiff(names(df_time_iter), [\"id_run\"; header_cols])], df_run_simulations_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcat(df_time_iter, df_run_simulations_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bfaeecedf253908c6d8da07a77979cd04405f6ff56d07cd432c822e25783765"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
