{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"C:/Users/Davide/git/gitdf/EnergyCommunity.jl/run_cloud\" #git/gitdf/EnergyCommunity.jl/run_cloud\"\n",
    "saveiter_dir = \"$parent_dir/results_paper/iter\"\n",
    "saveenum_dir = \"$parent_dir/results_paper/enum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using EnergyCommunity\n",
    "using FileIO\n",
    "using HiGHS, Plots\n",
    "using JuMP\n",
    "using Gurobi\n",
    "using Games\n",
    "using TickTock\n",
    "using Combinatorics\n",
    "using DataFrames\n",
    "using JLD2\n",
    "using Latexify, LaTeXStrings\n",
    "using YAML\n",
    "using CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare ENUM_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EC_size_list_enum = []\n",
    "dict_enums = Dict()\n",
    "\n",
    "for filename in readdir(saveenum_dir)\n",
    "    if endswith(filename, \".jld2\")\n",
    "        size_enum = parse(Int, replace(filename, \"enum_simulations_results_\"=>\"\", \".jld2\"=>\"\"))\n",
    "        push!(EC_size_list_enum, size_enum)\n",
    "        dict_enums[size_enum] = load(\"$saveenum_dir/enum_simulations_results_$size_enum.jld2\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENUM_MODE: Create reward redistribution of enum modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reward_enum = DataFrame()\n",
    "\n",
    "for EC_size in EC_size_list_enum\n",
    "    df_reward_temp = deepcopy(dict_enums[EC_size][\"df_reward_enum\"])\n",
    "    reward_list = setdiff(names(df_reward_temp), [\"user_set\"])\n",
    "    rename!(df_reward_temp, reward_list .=> reward_list .* \"_$EC_size\")\n",
    "    if nrow(df_reward_enum) == 0\n",
    "        df_reward_enum = df_reward_temp\n",
    "    else\n",
    "        df_reward_enum = outerjoin(df_reward_enum, df_reward_temp, on=:user_set, makeunique=true)\n",
    "    end\n",
    "end\n",
    "\n",
    "# sort by user set\n",
    "sort!(df_reward_enum, :user_set)\n",
    "\n",
    "df_reward_enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENUM_MODE: Create comparison of computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_enum = DataFrame()\n",
    "\n",
    "for EC_size in EC_size_list_enum\n",
    "    df_time_temp = DataFrame(dict_enums[EC_size][\"df_time_enum\"])  # dict_time_enum\n",
    "\n",
    "    df_time_temp[!, 2:end] ./= 3600  # change units to hours\n",
    "\n",
    "    df_time_temp[!, \"EC size\"] = [EC_size]\n",
    "    df_time_temp[!, \"title\"] = [L\"Time [h]\"]\n",
    "    df_time_temp = df_time_temp[!, [\"EC size\"; \"title\"; names(df_time_temp)[1:end-2]]]\n",
    "\n",
    "    if nrow(df_time_enum) == 0\n",
    "        df_time_enum = df_time_temp\n",
    "    else\n",
    "        df_time_enum = vcat(df_time_enum, df_time_temp)\n",
    "    end\n",
    "end\n",
    "\n",
    "df_time_enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare ITER_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_run_simulations_iter = CSV.read(\"$saveiter_dir/options_backup.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_iter = Dict()\n",
    "n_iter = 0\n",
    "\n",
    "while isfile(\"$saveiter_dir/iter_simulations_results_$(n_iter+1).jld2\")\n",
    "    n_iter += 1\n",
    "    dict_iter[n_iter] = load(\"$saveiter_dir/iter_simulations_results_$n_iter.jld2\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITER_MODE: Comparison of reward distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reward_iter = DataFrame()\n",
    "\n",
    "largest_user_set = []\n",
    "\n",
    "for id_run in 1:n_iter\n",
    "    df_reward_temp = deepcopy(dict_iter[id_run][\"df_reward_iter\"])\n",
    "    reward_list = setdiff(names(df_reward_temp), [\"user_set\"])\n",
    "    rename!(df_reward_temp, reward_list .=> reward_list .* \"_$(nrow(df_reward_temp)-1)\")\n",
    "    if nrow(df_reward_iter) == 0\n",
    "        df_reward_iter = df_reward_temp\n",
    "    else\n",
    "        df_reward_iter = outerjoin(df_reward_iter, df_reward_temp, on=:user_set, makeunique=true)\n",
    "    end\n",
    "\n",
    "    if length(largest_user_set) < nrow(df_reward_iter)\n",
    "        largest_user_set = df_reward_temp[!, \"user_set\"]\n",
    "    end\n",
    "end\n",
    "\n",
    "# sort by user set\n",
    "sort!(df_reward_iter, :user_set, by=x -> findfirst(x .== largest_user_set))\n",
    "CSV.write(\"reward.csv\", df_reward_iter)\n",
    "df_reward_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITER_MODE: Create comparison of computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_iter = DataFrame()\n",
    "\n",
    "for id_run in 1:n_iter\n",
    "    df_time_temp = deepcopy(dict_iter[id_run][\"df_time_iter\"])\n",
    "\n",
    "    df_time_temp[!, 3:end] ./= 3600  # change units to hours\n",
    "\n",
    "    df_time_temp[!, \"title\"] = [L\"Time [h]\"]\n",
    "\n",
    "    if nrow(df_time_iter) == 0\n",
    "        df_time_iter = df_time_temp\n",
    "    else\n",
    "        df_time_iter = vcat(df_time_iter, df_time_temp)\n",
    "    end\n",
    "end\n",
    "\n",
    "header_cols = [\"name\", names(df_run_simulations_iter)..., \"title\"]\n",
    "df_list_rewards = setdiff(names(df_time_iter), [\"id_run\"; header_cols])\n",
    "\n",
    "df_time_iter = hcat(df_run_simulations_iter[df_time_iter[!, \"id_run\"], :], df_time_iter[!, df_list_rewards])\n",
    "\n",
    "CSV.write(\"comp_time.csv\", df_time_iter)\n",
    "\n",
    "df_time_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITER_MODE: Create comparison of time per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history_iter = DataFrame()\n",
    "\n",
    "for id_run in 1:n_iter\n",
    "    df_history_temp = deepcopy(dict_iter[id_run][\"df_history\"])\n",
    "    df_history_temp[!, \"id_run\"] .= id_run\n",
    "\n",
    "    df_history_temp = df_history_temp[!, [\"id_run\"; setdiff(names(df_history_temp), [\"id_run\"])]]\n",
    "\n",
    "    if nrow(df_history_iter) == 0\n",
    "        df_history_iter = df_history_temp\n",
    "    else\n",
    "        df_history_iter = vcat(df_history_iter, df_history_temp)\n",
    "    end\n",
    "end\n",
    "\n",
    "df_history_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ((k,), v) in pairs(groupby(df_history_iter, [:name]))\n",
    "    # initialize plot\n",
    "    p = plot()\n",
    "\n",
    "    # loop over run ids\n",
    "    for ((k_r,), v_r) in pairs(groupby(v, [:id_run]))\n",
    "        plot!(v_r[!, :iteration], v_r[!, :elapsed_time], label=\"id_run: $k_r\")\n",
    "    end\n",
    "\n",
    "    xlabel!(\"Iteration [#]\")\n",
    "    ylabel!(\"Elapsed time [s]\")\n",
    "\n",
    "    display(p)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITER_MODE: Comparison of iteration time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ((k,), v) in pairs(groupby(df_history_iter, [:name]))\n",
    "    # initialize plot\n",
    "    p = plot()\n",
    "\n",
    "    # loop over run ids\n",
    "    for ((k_r,), v_r) in pairs(groupby(v, [:id_run]))\n",
    "        delta_time = [v_r[1, :elapsed_time]; v_r[2:end, :elapsed_time] .- v_r[1:end-1, :elapsed_time]]\n",
    "        plot!(v_r[!, :iteration], delta_time, label=\"id_run: $k_r\")\n",
    "    end\n",
    "\n",
    "    xlabel!(\"Iteration [#]\")\n",
    "    ylabel!(\"Elapsed time [s]\")\n",
    "\n",
    "    display(p)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITER_MODE: Comparison of upper and lower bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_plot = 3\n",
    "\n",
    "for ((k,), v) in pairs(groupby(df_history_iter, [:name]))\n",
    "    # initialize plot\n",
    "    # loop over run ids\n",
    "    for ((k_r,), v_r) in pairs(groupby(v, [:id_run]))\n",
    "        p = plot(v_r[!, :iteration], v_r[!, :value_min_surplus], label=\"Upper bound\", color=\"red\", title=\"$k - id_run: $k_r\")\n",
    "        plot!(v_r[!, :iteration], v_r[!, :lower_problem_min_surplus], label=\"Lower bound\", color=\"blue\")\n",
    "\n",
    "        best_obj = minimum(v_r[!, :value_min_surplus])\n",
    "\n",
    "        xlabel!(\"Iteration [#]\")\n",
    "        ylabel!(\"Obj. value\")\n",
    "        ylims!(p, -rng_plot*best_obj, rng_plot*best_obj)\n",
    "    \n",
    "        display(p)\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bfaeecedf253908c6d8da07a77979cd04405f6ff56d07cd432c822e25783765"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
